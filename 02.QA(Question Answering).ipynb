{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"F4C5t4J9nuBS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647698581869,"user_tz":-540,"elapsed":8629,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}},"outputId":"50fc7051-9a57-476b-8ec7-195c5e2e55c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 14.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 73.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 74.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"uvrQTvcDnYQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647699543106,"user_tz":-540,"elapsed":3725,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}},"outputId":"cc678af1-045d-4934-add3-79fad68f980a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","\n","import torch\n","from torch.optim import Adam, AdamW\n","from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n","from transformers import BertTokenizer, BertTokenizerFast, BertForQuestionAnswering\n","from transformers import get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import  f1_score\n","\n","import gc\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["%cd /content/gdrive/My Drive/NLP"],"metadata":{"id":"xvcZ380Em-al"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vQd9P24Cn1yb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647698608384,"user_tz":-540,"elapsed":2111,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}},"outputId":"730d394f-ad20-47ff-c637-cadf175c83bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-19 14:03:25--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n","Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 38527475 (37M) [application/json]\n","Saving to: ‘KorQuAD_v1.0_train.json’\n","\n","KorQuAD_v1.0_train. 100%[===================>]  36.74M  --.-KB/s    in 0.1s    \n","\n","2022-03-19 14:03:26 (313 MB/s) - ‘KorQuAD_v1.0_train.json’ saved [38527475/38527475]\n","\n","--2022-03-19 14:03:26--  https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n","Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3881058 (3.7M) [application/json]\n","Saving to: ‘KorQuAD_v1.0_dev.json’\n","\n","KorQuAD_v1.0_dev.js 100%[===================>]   3.70M  --.-KB/s    in 0.03s   \n","\n","2022-03-19 14:03:27 (137 MB/s) - ‘KorQuAD_v1.0_dev.json’ saved [3881058/3881058]\n","\n"]}],"source":["!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -O KorQuAD_v1.0_train.json\n","!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -O KorQuAD_v1.0_dev.json"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dmzqXLws2Wl1","executionInfo":{"status":"ok","timestamp":1647698608385,"user_tz":-540,"elapsed":3,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jNJWh_1LbCsH","executionInfo":{"status":"ok","timestamp":1647698608385,"user_tz":-540,"elapsed":3,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["def data_load(path):\n","    with open(path, 'rb') as f:\n","        squad_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = [] \n","    start_ids = []\n","    end_ids = []\n","\n","    for datas in squad_dict['data']:\n","        for paragraphs in datas['paragraphs']:\n","            context = paragraphs['context']\n","            for qas in paragraphs['qas']:\n","                question = qas['question']\n","                for answer in qas['answers']:\n","                    contexts.append(context)\n","                    questions.append(question)\n","                    answers.append(answer['text'])\n","\n","                    start_index = answer['answer_start']\n","                    start_ids.append(start_index)\n","\n","                    answer['text'] = answer['text'].rstrip()\n","                    \n","                    end_index = start_index + len(answer['text'])\n","                    end_ids.append(end_index)                  \n","\n","    return pd.DataFrame({'contexts' : contexts, 'questions' : questions, 'answers' : answers, 'start_ids': start_ids,'end_ids': end_ids})"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"4oQehYoVbShA","executionInfo":{"status":"ok","timestamp":1647698884188,"user_tz":-540,"elapsed":1121,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["train_df = data_load('KorQuAD_v1.0_train.json')\n","valid_df = data_load('KorQuAD_v1.0_dev.json')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"o6aIqQAKo4Na","executionInfo":{"status":"ok","timestamp":1647698970576,"user_tz":-540,"elapsed":320,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["def qa_preprocess(df, batch_size=16, method='train'):\n","  if method == 'train' or method == 'valid':    \n","      batch_input = tokenizer(df['contexts'].tolist(), df['questions'].tolist(), truncation=True, padding=True)\n","\n","      start_ids = df['start_ids'].tolist()\n","      end_ids = df['end_ids'].tolist()\n","\n","      start_positions = [batch_input.char_to_token(i, start_ids[i]) for i in range(len(start_ids))]\n","      end_positions = [batch_input.char_to_token(i, end_ids[i]-1) for i in range(len(end_ids))]\n","      deleting_list = [i for i, v in enumerate(end_positions) if v == None]\n","          \n","      batch_input.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","     \n","      batch_input = {key : np.delete(np.array(value), deleting_list, axis=0) for key, value in batch_input.items()}\n","      batch_input = {key : torch.tensor(value.astype(int)) for key, value in batch_input.items()}\n","\n","      dataset = TensorDataset(batch_input['input_ids'], batch_input['attention_mask'], batch_input['token_type_ids'], batch_input['start_positions'], batch_input['end_positions'])\n","      if method == 'train':\n","        dataset_sampler = RandomSampler(dataset)\n","        dataloader = DataLoader(dataset, sampler=dataset_sampler, batch_size=batch_size)\n","      elif method == 'valid':\n","        dataloader = DataLoader(dataset, batch_size=batch_size)\n","      return dataloader, deleting_list\n","\n","  elif method == 'test':\n","      batch_input = tokenizer(df['contexts'].tolist(), df['questions'].tolist(), truncation=True, padding=True)\n","      batch_input = {key : torch.tensor(value) for key, value in batch_input.items()}\n","\n","      dataset = TensorDataset(batch_input['input_ids'], batch_input['attention_mask'], batch_input['token_type_ids'])\n","      dataloader = DataLoader(dataset, batch_size=batch_size)\n","\n","  return dataloader"]},{"cell_type":"markdown","metadata":{"id":"DtCOpE3qpr-J"},"source":["# model training\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3jEr_ZSfqHRB","executionInfo":{"status":"ok","timestamp":1647698826704,"user_tz":-540,"elapsed":2,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["def train_one_epoch(optimizer, scheduler, dataloader):\n","    model.train()\n","\n","    train_loss = 0.0\n","\n","    for batchs in tqdm(dataloader):\n","        batch = tuple(b.to(device) for b in batchs)\n","\n","        inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"start_positions\": batch[3],\n","                \"end_positions\": batch[4],\n","            }\n","\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(**inputs)\n","        \n","        loss = outputs[0]\n","        \n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(dataloader)\n","    \n","    return avg_train_loss"]},{"cell_type":"markdown","metadata":{"id":"eIYgqtdZqHTE"},"source":["# model_evaluate"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QPEOSIRNrf2r","executionInfo":{"status":"ok","timestamp":1647698824651,"user_tz":-540,"elapsed":323,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"outputs":[],"source":["def evaluate_one_epoch(dataloader):\n","    model.eval()\n","\n","    start_preds = []\n","    end_preds = []\n","    input = []\n","\n","    for batchs in tqdm(dataloader):\n","        batch = tuple(b.to(device) for b in batchs)\n","\n","        inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2]\n","            }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        # CPU로 데이터 이동\n","        start_pred = outputs['start_logits'].detach().cpu()\n","        end_pred = outputs['end_logits'].detach().cpu()\n","\n","        input.append(inputs['input_ids'].detach().cpu())\n","        start_preds.append(start_pred)\n","        end_preds.append(end_pred)\n","\n","    input = torch.cat(input, dim=0).tolist()\n","    start_preds = torch.cat(start_preds, dim=0).argmax(dim=-1).tolist()\n","    end_preds = torch.cat(end_preds, dim=0).argmax(dim=-1).tolist()\n","\n","    answer = [tokenizer.decode(input[s:e+1]) for input, s, e in zip(input,start_preds,end_preds)]\n","\n","    return answer"]},{"cell_type":"markdown","metadata":{"id":"wGTP-kmurgcu"},"source":["# QA Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1UAY09vqHU1"},"outputs":[],"source":["def qa_model(train_data, dev_data,lr=1e-4,epochs = 4, batch_size=32, bert='klue/bert-base', save=True, path='bert_qa'):\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    global tokenizer\n","    tokenizer = BertTokenizerFast.from_pretrained(bert)\n","\n","    global model,device\n","    model = BertForQuestionAnswering.from_pretrained(bert)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    train_dataloader,train_delete = qa_preprocess(train_data, batch_size=batch_size, method = 'train')\n","    val_dataloader,val_delete = qa_preprocess(dev_data, batch_size=batch_size, method = 'valid')\n","    print('')\n","    print('Preprocess Compelete')\n","    print('')\n","\n","    # no_decay = ['bias', 'LayerNorm.weight']\n","    # optimizer_grouped_parameters = [\n","    #     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","    #         'weight_decay': 0.01},\n","    #     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    # ]\n","\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n","\n","    total_steps = len(train_dataloader) * epochs\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0,\n","                                                num_training_steps = total_steps)\n","\n","    print('')\n","    print('Model Training Start')\n","    print(f'Epochs : {epochs} / Learning Rate : {str(lr)} / Batch Size : {batch_size}')\n","    print('')\n","\n","    loss = []\n","    f1 = []\n","    for epoch in range(1,epochs+1):\n","        print(f\"epoch = {epoch}\")\n","\n","        loss = train_one_epoch(optimizer, scheduler, dataloader=train_dataloader)\n","        preds = evaluate_one_epoch(dataloader=val_dataloader)\n","\n","        f1_score = f1_score(valid_df.drop(val_delete,axis=0)['answers'].tolist(),preds,average='micro')\n","        f1.append(f1_score)\n","\n","        print('')\n","\n","        if save:\n","            model.save_pretrained(f'models/{path}')\n","            tokenizer.save_pretrained(f'models/{path}')\n","            print('Model Save')\n","    \n","    print('')\n","    print(\"Training Complete!\")\n","\n","    return {'loss':loss,'f1 score':f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nxL1ftMq8vG"},"outputs":[],"source":["score,preds = qa_model(train_df, valid_df, lr = 1e-5,epochs=1,batch_size=16, bert='klue/bert-base')"]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"nsIduLJJfZX2"}},{"cell_type":"code","source":["def QA_test(test_data, batch_size=32,bert='QA'):\n","    global tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(f'klue/bert-base')\n","    test_dataloader = qa_preprocess(test_data, batch_size=batch_size, method='test')\n","\n","    global model, device\n","    model = BertForQuestionAnswering.from_pretrained(f'models/{bert}')\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    answer = evaluate_one_epoch(dataloader=test_dataloader)\n","\n","    new_df = valid_df[['contexts','questions']].copy()\n","    new_df['answers'] = answer\n","    \n","    return  new_df"],"metadata":{"id":"hfZ_SA2NWeQx","executionInfo":{"status":"ok","timestamp":1647699608027,"user_tz":-540,"elapsed":325,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["df = QA_test(valid_df, batch_size=32,bert='QA')"],"metadata":{"id":"IqEhMmcEUJ4g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647699687435,"user_tz":-540,"elapsed":50075,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}},"outputId":"56a1fa9f-769d-46f6-8bfe-fbbdd5686fe3"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 181/181 [00:54<00:00,  3.31it/s]\n"]}]},{"cell_type":"code","source":["sample = df.sample(5)\n","\n","for i in range(len(sample)):\n","    print('context : ', sample['contexts'].tolist()[i])\n","    print('questions : ', sample['questions'].tolist()[i])\n","    print('answers : ', sample['answers'].tolist()[i])\n","    print('')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adMSENuNhkDu","executionInfo":{"status":"ok","timestamp":1647700532227,"user_tz":-540,"elapsed":360,"user":{"displayName":"박정호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggmkpfk7ypFS-awqdcEb0eV-KsaPVpj6ru8vni6rQ=s64","userId":"04165485851555310341"}},"outputId":"26bab169-b90d-4a76-c19a-0ca5240dfd4b"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["context :  고종의 정비로 1871년 첫 왕자를 5일 만에 잃고, 최익현 등과 손잡고 흥선대원군의 간섭을 물리치고 고종의 친정을 유도했다. 민씨 척족을 기용함으로써 세도정권을 부활시켰으며, 1882년 임오군란 이후 일본한테 겨냥하는 견제를 위해 청나라의 지원에 의존하다가 1894년 청일전쟁에서 청나라가 패배당한 이후에는 러시아를 끌어들여 일본을 견제했다. 맨 처음에는 개항에 미온적이었으나, 점진적인 개화시책을 통해 친일 성향을 띤 급진 개화파의 개화정책에 제동을 걸었다. 그러다가 흥선대원군과 주조선 일본공사 미우라 고로의 공모에 의해 일본인 병사와 낭인들에게 암살당했다(을미사변(乙未事變), 1895년). 사후 대한제국이 성립되면서 황후로 추봉되었다. 정식 시호는 효자원성정화합천홍공성덕제휘열목명성태황후(孝慈元聖正化合天洪功誠德齊徽烈穆明成太皇后)이다.\n","questions :  명성황후가 암살당한 해는?\n","answers :  1895년\n","\n","context :  페니키아(그리스어: Φοινίκη, 라틴어: Phœnicia)는 고대 가나안의 북쪽에 근거지를 둔 고대 문명이다. 중심 지역은 오늘날의 레바논과 시리아, 이스라엘 북부로 이어지는 해안에 있었다. 페니키아 문명은 기원전 1200년경에서 900년경까지 지중해를 가로질러 퍼져나간 진취적인 해상 무역 문화를 가졌다고 과거의 고대사엔 수록되어 있지만. 최근의 고고학 발굴로 페니키아 문명은 그보다 훨씬 오래전인 기원전 40세기에 '기시 문명'이 지중해 문명과 메소포타미아를 연계하는 문명이라고 한다. 학자들은 이 '기시 문명(Gish)'이 아프리카의 '쿠마 문화(Kuma), 나일 강 유역의 쿠시 문화(Kush)와 연결고리가 있으며 그 관계를 연구중이라 한다. 연구된 바에 의하면 북아프리카의 튀니지(고대 카르타고)를 중심으로 알제리, 리비아 등 북아프리카권역의 국가와 무역으로 이탈리아의 중, 남부 도시에 식민지를 건설하였으며 리베리아(현 스페인)의 항구도시에 식민지를 건설하였다는 것은 이집트 기록에 의한 것이다. 고대의 경계가 변동하긴 했지만, 도시의 문화 중심은 최남단으로 도시 티레로 여겨진다. 페니키아 본토의 도시 가운데 시돈과 티레 사이에 있는 사렙타는 가장 완벽하게 발굴된 도시이다. 페니키아는 최초로 갤리선을 사용한 문명으로 이들은 갤리선을 이용한 무역으로 번성하였다.\n","questions :  쿠마 문화와 쿠시 문화의 연결고리로 추정되는 문명은?\n","answers :  기시 문명\n","\n","context :  압구정 현대아파트는 1976년 현대산업개발에서 건설한 아파트단지이다. 강남개발이 진행되면서 강남구는 인구가 집중되면서 주거시설이 늘고 도시가 팽창하기 시작했다. 지금은 ‘강남’이라고 불리는 영동지구를 중심으로 도시 성장과 변화의 물결이 한강변을 따라 퍼져나갔다. 아파트는 고도의 경제성장과 공간 팽창을 널리 알리는 상징으로 작용했다. 엘리베이터와 수세식 화장실을 갖춘 고층 아파트는 성장의 흔적을 압축적으로 보여줬다. 1969년 만든 한남대교와 1970년 개통한 경부고속도로는 영동지구 개발과 아파트 건설의 속도를 더하여 현대산업개발에게 아파트 건설을 할 수 있는 자본을 주었다. 1975년 강남구가 탄생했고, 1976년에는 반포동 압구정동 청담동 도곡동이 ‘아파트 지구’로 지정됐다. 압구정동 현대아파트는 혁신적인 디자인이나 고급스러운 시설을 도입한 건물이 아니지만 이곳이 한국에서 손꼽히는 고급 아파트 단지로 자리 잡은 것은 1978년 7월의 특혜분양 사건 때문이었다. 당시 건설사는 ‘50가구 이상의 주택을 건설하는 사업자는 공개 분양해야 한다’는 주택건설촉진법을 무시하고 건설한 아파트의 상당수를 정부 관리, 국회의원, 대학교수 등 고위급 인사들에게 주변 집값의 50% 수준으로 특혜 분양했다. 분양과 동시에 약 5000만원의 프리미엄이 형성됐다. 분양가는 3.3m²당 44만 원. 5000만 원은 당시 현대아파트 115m² 1채의 분양가에 해당했다. 160m² 이상의 대형 아파트를 국내에 처음으로 선보인 이 아파트 이후 건설회사의 이름을 따른 아파트 이름이 유행처럼 늘어났다. 1980년대 후반에는 현대아파트를 중심으로 압구정동 고소득층 주거지에 커다란 문화적 변화의 물결이 일었다. 고소득층 아파트의 상징이 된 압구정동 현대아파트는 30여 년이 흐른 지금도 국내에서 가장 비싼 아파트 가운데 하나다. 서울시의 한강변 정비 계획과 강변 아파트 촉진 계획에 따라 재건축 논의가 이루어지고 있으나 아직 구체적인 방안은 나오고 있지는 않다.\n","questions :  압구정 현대아파트를 건설한 회사는?\n","answers :  현대산업개발\n","\n","context :  그러나 군사정권 출신과 일부 보수 세력에서는 그의 등장을 인정하기를 거부했다. 김영삼의 사생활이 본격적으로 도마에 오른 것은 1992년 민자당 대선후보 때였다. 그 해 2월 20일자 LA매일신문에 '김영삼 씨의, 숨겨둔 딸 가오리, 뉴욕에 거주하고 있다.'는 기사가 실린 것을 시작으로 국내외 언론에서 동시다발적인 보도가 나왔다. 이 과정에서 LA매일신문 발행인 손충무가 긴급 구속되기도 했다. 당시 국내에서는 'ys의 숨겨둔 딸 가오리 양'의 이야기가 널리 회자화됐다. 나중에는 의혹이 눈덩이처럼 커져 \"숨겨진 딸뿐만 아니라 아들도 있다더라\"는 소문도 나돌았다. 군사 정권 출신 인사들은 이를 호재삼아 김영삼을 비난하는 근거로 활용했다.\n","questions :  그의 사생활 폭로를 한 신문사는?\n","answers :  LA매일신문\n","\n","context :  본선 조별리그에서는 자신들과 똑같은 처지로 한때는 강자였으나 쇠락한 프랑스와 비기고 나머지 두 팀인 멕시코와 개최국 남아프리카공화국을 연파하고 16강에 안착했다. 16강에서는 대한민국과 매우 힘겨운 대결 끝에 간신히 8강에 진출했는데 실제로도 이 경기를 뛰었던 디에고 포를란은 2010년 월드컵에서 맞붙은 상대 중에서 한국이 제일 어려웠다고 회고했다. 8강에서는 가나를 상대로 연장 후반에 실점할 위기에 몰렸으나 루이스 알베르토 수아레스가 손으로 슈팅을 쳐내서 퇴장당했고 곧바로 패널티킥으로 이어졌다. 아사모아 기안이 찬 패널티킥이 골대 윗쪽을 맞고 경기장 밖으로 나가면서 양 팀은 승부차기에 돌입하게 되었으며 이 승부차기에서 이겨서 우루과이는 장장 40년 만에 4강에 진출했다. 하지만 4강 이후에 만나는 팀들이 다 그렇듯 엄청나게 어려웠다. 결국 4강에서 네덜란드와 3/4위전에서 독일에게 각각 2-3으로 패하고 4위를 차지했다.\n","questions :  우루과이와 8강에서의 상대국가는? \n","answers :  대한민국과 매우 힘겨운 대결 끝에 간신히 8강에 진출했는데 실제로도 이 경기를 뛰었던 디에고 포를란은 2010년 월드컵에서 맞붙은 상대 중에서 한국이 제일 어려웠다고 회고했다. 8강에서는 가나\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"03-QA(Question Answering).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}